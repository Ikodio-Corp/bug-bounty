\documentclass[10pt,twocolumn]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[margin=0.75in]{geometry}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\setlength{\columnsep}{0.25in}

\begin{document}

\section{Future Research Directions}

Online learning with concept drift detection (ADWIN) enabling retraining when characteristics shift (elections, recessions, policy pivots). (5) Explainable AI: Attention visualization and SHAP values quantifying component contributions for transparency and regulatory compliance. Additional directions include transformer architectures (Temporal Fusion Transformers), cross-asset correlation (equities/bonds/currencies), longitudinal studies across market cycles, and A/B testing measuring actual trading profitability beyond backtested metrics.

\section{Acknowledgment}

This research and system development were conducted jointly by Hylmi Rafif Rabbani and Ali Gunawan at the Department of Information Systems, Bina Nusantara University. Both authors contributed equally to the conceptualization, implementation, and evaluation of the multi-modal stock prediction platform. H.R. Rabbani led the technical architecture design and LSTM implementation, while A. Gunawan directed the integration of sentiment analysis and production deployment. We thank the anonymous reviewers for their constructive feedback that improved this manuscript.

\begin{thebibliography}{20}

\bibitem{idx2023}
Indonesia Stock Exchange, ``IDX Annual Statistics 2023,'' Jakarta, Indonesia, 2023. [Online]. Available: https://www.idx.co.id/

\bibitem{zhang2021}
J. Zhang, S. Ye, and L. Xing, ``A novel data-driven stock price trend prediction system,'' \textit{Expert Systems with Applications}, vol. 97, pp. 60--69, May 2021.

\bibitem{wilder1978}
J. W. Wilder, \textit{New Concepts in Technical Trading Systems}. Greensboro: Trend Research, 1978.

\bibitem{fischer2018}
T. Fischer and C. Krauss, ``Deep learning with long short-term memory networks for financial market predictions,'' \textit{European Journal of Operational Research}, vol. 270, no. 2, pp. 654--669, Oct. 2018.

\bibitem{chen2015}
K. Chen, Y. Zhou, and F. Dai, ``A LSTM-based method for stock returns prediction: A case study of China stock market,'' in \textit{Proc. IEEE Int. Conf. Big Data}, 2015, pp. 2823--2824.

\bibitem{liu2023}
Y. Liu, Q. Qin, and H. Xu, ``News sentiment analysis for stock market prediction: A deep learning approach,'' \textit{Information Processing \& Management}, vol. 60, no. 3, article 103307, May 2023.

\bibitem{lopez2023}
M. Lopez-Lira and Y. Tang, ``Can ChatGPT forecast stock price movements? Return predictability and large language models,'' \textit{arXiv preprint arXiv:2304.07619}, Apr. 2023.

\bibitem{xu2018}
Y. Xu and S. B. Cohen, ``Stock movement prediction from tweets and historical prices,'' in \textit{Proc. 56th ACL}, 2018, pp. 1970--1979.

\bibitem{gao2021}
Y. Gao, R. Wang, and E. Zhou, ``Stock prediction based on optimized LSTM and GRU models,'' \textit{Scientific Programming}, vol. 2021, article 4055281, 2021.

\bibitem{huang2023}
S. Huang, T. K. Ho, and N. H. Chieu, ``Multi-modal fusion for stock price prediction using deep learning,'' \textit{IEEE Access}, vol. 11, pp. 23847--23859, 2023.

\bibitem{li2022}
X. Li, H. Xie, R. Wang, et al., ``Empirical analysis: Stock market prediction via extreme learning machine,'' \textit{Neural Computing and Applications}, vol. 34, no. 4, pp. 2943--2967, Feb. 2022.

\bibitem{sezer2020}
O. B. Sezer, M. U. Gudelek, and A. M. Ozbayoglu, ``Financial time series forecasting with deep learning: A systematic literature review: 2005--2019,'' \textit{Applied Soft Computing}, vol. 90, article 106181, May 2020.

\bibitem{jiang2021}
W. Jiang, ``Applications of deep learning in stock market prediction: Recent progress,'' \textit{Expert Systems with Applications}, vol. 184, article 115537, Dec. 2021.

\bibitem{lecun2015}
Y. LeCun, Y. Bengio, and G. Hinton, ``Deep learning,'' \textit{Nature}, vol. 521, no. 7553, pp. 436--444, May 2015.

\bibitem{hochreiter1997}
S. Hochreiter and J. Schmidhuber, ``Long short-term memory,'' \textit{Neural Computation}, vol. 9, no. 8, pp. 1735--1780, Nov. 1997.

\bibitem{socher2013}
R. Socher, A. Perelygin, J. Wu, et al., ``Recursive deep models for semantic compositionality over a sentiment treebank,'' in \textit{Proc. EMNLP}, 2013, pp. 1631--1642.

\bibitem{vaswani2017}
A. Vaswani, N. Shazeer, N. Parmar, et al., ``Attention is all you need,'' in \textit{Proc. NIPS}, 2017, pp. 6000--6010.

\bibitem{gal2016}
Y. Gal and Z. Ghahramani, ``Dropout as a Bayesian approximation: Representing model uncertainty in deep learning,'' in \textit{Proc. ICML}, 2016, pp. 1050--1059.

\bibitem{chen2016}
T. Chen and C. Guestrin, ``XGBoost: A scalable tree boosting system,'' in \textit{Proc. ACM SIGKDD}, 2016, pp. 785--794.

\bibitem{lim2021}
B. Lim, S. Ö. Arık, N. Loeff, and T. Pfister, ``Temporal fusion transformers for interpretable multi-horizon time series forecasting,'' \textit{Int. Journal of Forecasting}, vol. 37, no. 4, pp. 1748--1764, Oct.--Dec. 2021.

\end{thebibliography}

\end{document}
